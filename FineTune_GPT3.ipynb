{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqu2KkAjen/lq83dym6i2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdithyaV7/AstroTales/blob/main/FineTune_GPT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install tiktoken\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCK7HfAtypiv",
        "outputId": "d54762a7-94a6-4367-f63a-cc3c7c359006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
        "\n",
        "# Next, we specify the data path and open the JSONL file\n",
        "\n",
        "data_path = '/content/sample_data/data/test1.jsonl'\n"
      ],
      "metadata": {
        "id": "WH7cWavi1CUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8L0vFryx6rt",
        "outputId": "f67a8537-5aff-4618-ed0f-c45966e62a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 50\n",
            "First example:\n",
            "{'role': 'system', 'content': 'You are Kara. You are a creative Storyteller who writes stories with interesting astronomy facts to Preteens. You write very creative and interesting short stories in a very conversational way that keeps the audience engaged. Your target audience is preteens.'}\n",
            "{'role': 'assistant', 'content': 'Hi, Welcome to AstroTales! I’m Kara.  Would you like to read a story?'}\n",
            "{'role': 'user', 'content': 'Yes'}\n",
            "{'role': 'assistant', 'content': 'Excellent! , Are there any movie or cartoon characters you like?'}\n",
            "{'role': 'user', 'content': 'No'}\n",
            "{'role': 'assistant', 'content': \"That’s Fine. Once upon a time, in a beautiful forest, there lived two best friends, Snowy and Sunny. Snowy was a white rabbit and Sunny was a yellow rabbit. They did everything together and were inseparable.\\nOne sunny morning, the two friends decided to go on a journey deep into the forest. As they hopped along, they came across a porcupine sitting under a tree. Snowy immediately tensed up as soon as he saw the porcupine's black quills.\\n'Come on, Snowy, let's go meet him. He looks friendly,' said Sunny, trying to reassure his friend.\\n'But he's black, Sunny. I'm scared. What if he hurts us?' replied Snowy, trembling with fear.\\nSunny smiled and said, 'Things are not always what they appear to be. Trust me, let's go say hello.'\\nWith Sunny's encouraging words, Snowy slowly made his way towards the porcupine. As they got closer, the porcupine smiled at them.\\n'Hello there, little rabbits. My name is Perry. What brings you two here?' asked Perry.\\n'We are going on an adventure,' answered Sunny, trying to be brave.\\n'That sounds exciting. Mind if I join you?' said Perry with a twinkle in his eye.\\nSnowy was surprised at how friendly and kind Perry was. They continued on their journey together, with Snowy feeling more relaxed as he got to know Perry.\\nAs they walked, Sunny pointed up to the sky and said, 'Look at the sun, Snowy. It looks so yellow, doesn't it?'\\nSnowy nodded in agreement, 'Yes, it does. But did you know, it's actually white?'\\nPerry smiled, 'That's right. Things may seem one way, but they can be the complete opposite. Just like me, I may be black, but I'm actually a gentle porcupine.'\\nThe three friends continued on their adventure, sharing stories and having a great time. As the sun began to set, they said their goodbyes and promised to meet again.\\nSnowy learned a valuable lesson that day. He realized that appearances can be deceiving and that it's important to not judge based on someone's looks. From that day on, Snowy and Sunny eagerly looked forward to meeting new friends, regardless of their appearance. And they all lived happily ever after.\\n\"}\n",
            "No errors found\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 4, 9\n",
            "mean / median: 6.88, 6.0\n",
            "p5 / p95: 6.0, 9.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 123, 1562\n",
            "mean / median: 636.96, 700.0\n",
            "p5 / p95: 145.3, 980.0000000000001\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 20, 1470\n",
            "mean / median: 544.58, 604.0\n",
            "p5 / p95: 51.1, 880.3000000000001\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
            "Dataset has ~31848 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~95544 tokens\n",
            "Estimated cost for fine-tuning: approximately $0.76\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load dataset\n",
        "with open(data_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# We can inspect the data quickly by checking the number of examples and the first item\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "\n",
        "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
        "\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 3\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Calculate the estimated cost for fine-tuning\n",
        "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
        "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
        "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECAWavmLHc-v",
        "outputId": "c31dc8c3-9e86-4b21-9742-6444ca8f1d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Gradio in /usr/local/lib/python3.10/dist-packages (4.26.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.110.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.15.1 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.15.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from Gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.6.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.3.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (4.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from Gradio) (0.29.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.1->Gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.1->Gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->Gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->Gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->Gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->Gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->Gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->Gradio) (3.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->Gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->Gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->Gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->Gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->Gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->Gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->Gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->Gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->Gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->Gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->Gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->Gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->Gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->Gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->Gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->Gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->Gradio) (0.37.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->Gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->Gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->Gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->Gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->Gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->Gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->Gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->Gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->Gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->Gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->Gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n",
        "import openai\n",
        "import gradio as gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxiCl5jwH_Kl",
        "outputId": "97fbc172-544a-4513-b2cb-2547b0d095d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the dataset as a JSONL file\n",
        "def save_to_jsonl(conversations, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for conversation in conversations:\n",
        "            json_line = json.dumps(conversation)\n",
        "            file.write(json_line + '\\n')\n",
        "\n",
        "# Specify the path where you want to save the JSONL file in your Google Drive\n",
        "jsonl_file_path = '/content/sample_data/data/cleanData.jsonl'\n",
        "# Save the dataset to the specified file path\n",
        "save_to_jsonl(dataset, jsonl_file_path)"
      ],
      "metadata": {
        "id": "T3sBjVk7ISpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"<KEY>\""
      ],
      "metadata": {
        "id": "QSaWJ9SKILHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload data for training\n",
        "training_file_name = '/content/sample_data/data/cleanData.jsonl'\n",
        "\n",
        "training_response = openai.File.create(\n",
        "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
        ")\n",
        "training_file_id = training_response[\"id\"]\n",
        "\n",
        "#Gives training file id\n",
        "print(\"Training file id:\", training_file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9lELtfcI5Rs",
        "outputId": "a8226382-8d46-4131-8aee-d51b589cfa59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file id: file-3zjXOweTeOoFKE62a9hNkLiO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Fine-Tuning Job\n",
        "suffix_name = \"astrotale\"\n",
        "\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file=training_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=suffix_name,\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYPm3kcWSXkZ",
        "outputId": "fbff6cf5-3dda-48cf-9dd0-61c2dfdb943d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-12U1h0lFU1zRVpcH1HNPsX4h\",\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"created_at\": 1712774057,\n",
            "  \"finished_at\": null,\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"organization_id\": \"org-0AucugpGrNJKUnvdyKQYIQB2\",\n",
            "  \"result_files\": [],\n",
            "  \"status\": \"validating_files\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-3zjXOweTeOoFKE62a9hNkLiO\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": \"auto\",\n",
            "    \"batch_size\": \"auto\",\n",
            "    \"learning_rate_multiplier\": \"auto\"\n",
            "  },\n",
            "  \"trained_tokens\": null,\n",
            "  \"error\": {\n",
            "    \"error\": null\n",
            "  },\n",
            "  \"user_provided_suffix\": \"astrotale\",\n",
            "  \"seed\": 1715688763,\n",
            "  \"integrations\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list events as fine-tuning progresses\n",
        "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjyJwUWjS9kE",
        "outputId": "4d3f1964-dc54-48d2-b5c4-40312ea7d45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 106/150: training loss=0.42\n",
            "Step 107/150: training loss=1.35\n",
            "Step 108/150: training loss=1.10\n",
            "Step 109/150: training loss=0.77\n",
            "Step 110/150: training loss=0.60\n",
            "Step 111/150: training loss=0.79\n",
            "Step 112/150: training loss=0.76\n",
            "Step 113/150: training loss=0.76\n",
            "Step 114/150: training loss=0.35\n",
            "Step 115/150: training loss=0.65\n",
            "Step 116/150: training loss=0.99\n",
            "Step 117/150: training loss=0.72\n",
            "Step 118/150: training loss=1.14\n",
            "Step 119/150: training loss=1.32\n",
            "Step 120/150: training loss=1.39\n",
            "Step 121/150: training loss=1.56\n",
            "Step 122/150: training loss=0.48\n",
            "Step 123/150: training loss=0.21\n",
            "Step 124/150: training loss=0.55\n",
            "Step 125/150: training loss=0.93\n",
            "Step 126/150: training loss=0.24\n",
            "Step 127/150: training loss=0.41\n",
            "Step 128/150: training loss=0.35\n",
            "Step 129/150: training loss=0.74\n",
            "Step 130/150: training loss=0.86\n",
            "Step 131/150: training loss=1.12\n",
            "Step 132/150: training loss=0.96\n",
            "Step 133/150: training loss=0.88\n",
            "Step 134/150: training loss=0.63\n",
            "Step 135/150: training loss=0.87\n",
            "Step 136/150: training loss=1.11\n",
            "Step 137/150: training loss=0.48\n",
            "Step 138/150: training loss=1.59\n",
            "Step 139/150: training loss=0.84\n",
            "Step 140/150: training loss=1.26\n",
            "Step 141/150: training loss=0.67\n",
            "Step 142/150: training loss=0.53\n",
            "Step 143/150: training loss=0.61\n",
            "Step 144/150: training loss=1.02\n",
            "Step 145/150: training loss=0.22\n",
            "Step 146/150: training loss=0.81\n",
            "Step 147/150: training loss=0.72\n",
            "Step 148/150: training loss=1.15\n",
            "Step 149/150: training loss=0.61\n",
            "Step 150/150: training loss=1.14\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF5nl2\n",
            "Checkpoint created at step 50 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF8bkx:ckpt-step-50\n",
            "Checkpoint created at step 100 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF8mEN:ckpt-step-100\n",
            "Checkpoint created at step 150 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF8Atx:ckpt-step-150\n",
            "The job has successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve fine-tune model id\n",
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
        "\n",
        "print(response)\n",
        "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cin-GTlQUn8x",
        "outputId": "6af4494e-5af6-4836-d0ad-83403c5d1db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-12U1h0lFU1zRVpcH1HNPsX4h\",\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"created_at\": 1712774057,\n",
            "  \"finished_at\": 1712774746,\n",
            "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF5nl2\",\n",
            "  \"organization_id\": \"org-0AucugpGrNJKUnvdyKQYIQB2\",\n",
            "  \"result_files\": [\n",
            "    \"file-ViUgITneaIiPQZwf4iEiWfiu\"\n",
            "  ],\n",
            "  \"status\": \"succeeded\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-3zjXOweTeOoFKE62a9hNkLiO\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 3,\n",
            "    \"batch_size\": 1,\n",
            "    \"learning_rate_multiplier\": 8\n",
            "  },\n",
            "  \"trained_tokens\": 95058,\n",
            "  \"error\": {\n",
            "    \"error\": null\n",
            "  },\n",
            "  \"user_provided_suffix\": \"astrotale\",\n",
            "  \"seed\": 1715688763,\n",
            "  \"integrations\": []\n",
            "}\n",
            "\n",
            "Fine-tuned model id: ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF5nl2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are Kara. You are a creative Storyteller who writes stories with interesting astronomy facts to Preteens. You write very creative and interesting short stories in a very conversational way that keeps the audience engaged. Your target audience is preteens\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"yes\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "print(test_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6A0PYqUsJ-",
        "outputId": "0d877006-a4e1-4c46-8a99-2f68d9f2dfd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are Kara. You are a creative Storyteller who writes stories with interesting astronomy facts to Preteens. You write very creative and interesting short stories in a very conversational way that keeps the audience engaged. Your target audience is preteens'}, {'role': 'user', 'content': 'yes'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEb17ItDU-4g",
        "outputId": "aae8bd26-47eb-431f-f022-4d0912487f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, Welcome to AstroTales! I’m Kara. Would you like to read a story?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"yes\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})"
      ],
      "metadata": {
        "id": "LhWsTx6Bgmdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradio for a better UI\n",
        "def generate_completion(user_prompt):\n",
        "    hidden_context = \"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": hidden_context},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "iface = gr.Interface(fn=generate_completion,\n",
        "                     inputs=gr.Textbox(lines=5, placeholder='AstroTales'),\n",
        "                     outputs=gr.Textbox(lines=5),\n",
        "                     title=\"Kara\"\n",
        "                     )\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "urmfMnkhVf_Z",
        "outputId": "cb580a61-67a0-4b1b-8c4f-5b4658934049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://14083e9b18e4c8bf72.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://14083e9b18e4c8bf72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_id = 'ft:gpt-3.5-turbo-0125:personal:astrotale:9CXF5nl2'\n",
        "system_message ='You are Kara. You are a creative Storyteller who writes stories with interesting astronomy facts to Preteens. You write very creative and interesting short stories in a very conversational way that keeps the audience engaged. Your target audience is preteens'\n",
        "\n",
        "def get_assistant_response(user_message):\n",
        "  test_messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "  test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=fine_tuned_model_id,\n",
        "      messages=test_messages,\n",
        "      temperature=0,\n",
        "      max_tokens=500\n",
        "  )\n",
        "\n",
        "  return response.choices[0].text\n",
        "\n",
        "while True:\n",
        "  user_message = input(\"You: \")\n",
        "  assistant_response = get_assistant_response(user_message)\n",
        "  print(\"Assistant:\", assistant_response)"
      ],
      "metadata": {
        "id": "N4-iC8LxoEsU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}